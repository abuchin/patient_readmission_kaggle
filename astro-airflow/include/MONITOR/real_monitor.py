#!/usr/bin/env python3
"""
Real monitoring script that applies the same preprocessing pipeline
as used during model training to enable real ML service predictions.
"""
import os
import sys
import pandas as pd
import numpy as np
import json

# Add the MONITOR directory to the path so we can import monitor_and_retrain
monitor_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, monitor_dir)

import monitor_and_retrain

def get_preprocessing_pipeline():
    """Create the same preprocessing pipeline used during training"""
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    from sklearn.compose import ColumnTransformer
    
    # These column definitions should match what was used during training
    # We'll determine them dynamically from the data
    def create_preprocessor(X):
        numeric_cols = X.select_dtypes(include=["number"]).columns
        categorical_cols = X.select_dtypes(include=["object", "category", "bool"]).columns
        
        preprocessor = ColumnTransformer(
            transformers=[
                ("num", StandardScaler(), numeric_cols),
                ("cat", OneHotEncoder(drop="first", handle_unknown="ignore", sparse_output=False), categorical_cols),
            ],
            remainder="drop",
        )
        return preprocessor, numeric_cols, categorical_cols
    
    return create_preprocessor

def prepare_data_for_prediction(df, fit_preprocessor=None):
    """
    Prepare raw data for ML service prediction by applying the same preprocessing
    pipeline used during training.
    
    Args:
        df: Raw dataframe with categorical and numeric features
        fit_preprocessor: Pre-fitted preprocessor (if available), or None to fit on this data
    
    Returns:
        Tuple of (preprocessed_data_csv_string, fitted_preprocessor)
    """
    # Remove target columns if present
    feature_df = df.copy()
    target_cols = ['readmitted', 'readmit_binary']  # Remove known target columns
    for col in target_cols:
        if col in feature_df.columns:
            feature_df = feature_df.drop(columns=[col])
    
    create_preprocessor = get_preprocessing_pipeline()
    
    if fit_preprocessor is None:
        # Create and fit new preprocessor
        preprocessor, numeric_cols, categorical_cols = create_preprocessor(feature_df)
        preprocessor.fit(feature_df)
        print(f"ğŸ”§ Created preprocessor with {len(numeric_cols)} numeric and {len(categorical_cols)} categorical features")
    else:
        preprocessor = fit_preprocessor
    
    # Transform the data
    try:
        X_processed = preprocessor.transform(feature_df)
        print(f"âœ… Preprocessed data: {X_processed.shape[0]} rows Ã— {X_processed.shape[1]} features")
        
        # Convert to DataFrame with generic column names for CSV
        feature_names = [f"feature_{i}" for i in range(X_processed.shape[1])]
        processed_df = pd.DataFrame(X_processed, columns=feature_names)
        
        # Convert to CSV string
        csv_string = processed_df.to_csv(index=False)
        return csv_string, preprocessor
        
    except Exception as e:
        print(f"âŒ Preprocessing failed: {e}")
        raise

def score_with_preprocessing(df, endpoint, tmp_csv_path, pred_col="pred_readmit"):
    """
    Score data with preprocessing applied.
    Returns the original dataframe with predictions attached.
    """
    # Prepare data for prediction
    csv_data, _ = prepare_data_for_prediction(df)
    
    # Write preprocessed data to temporary file
    with open(tmp_csv_path, 'w') as f:
        f.write(csv_data)
    
    print(f"ğŸ“ Preprocessed data written to: {tmp_csv_path}")
    
    # Call the ML service
    try:
        raw_predictions = monitor_and_retrain.post_csv(endpoint, tmp_csv_path)
        print(f"ğŸ“¡ ML service response type: {type(raw_predictions)}")
        
        # Convert predictions to float list
        predictions = monitor_and_retrain._coerce_to_float_list(raw_predictions)
        print(f"ğŸ¯ Received {len(predictions)} predictions")
        
        if len(predictions) != len(df):
            raise ValueError(f"Prediction count {len(predictions)} != data rows {len(df)}")
        
        # Attach predictions to original dataframe
        result_df = df.copy()
        result_df[pred_col] = predictions
        
        return result_df
        
    except Exception as e:
        print(f"âŒ ML service call failed: {e}")
        print("ğŸ”„ Falling back to mock predictions for drift analysis...")
        
        # Fallback to mock predictions
        np.random.seed(42)
        mock_predictions = np.random.beta(2, 5, len(df))
        result_df = df.copy()
        result_df[pred_col] = mock_predictions
        
        return result_df

def main():
    """Run monitoring with real ML service predictions"""
    import argparse
    
    ap = argparse.ArgumentParser(description="Real monitoring script with preprocessing")
    ap.add_argument("--baseline", required=True, help="Baseline CSV")
    ap.add_argument("--current", required=True, help="Current CSV")
    ap.add_argument("--endpoint", required=True, help="ML endpoint")
    ap.add_argument("--tracking-uri", help="MLflow tracking URI")
    ap.add_argument("--retrain-script", help="Retrain script path")
    ap.add_argument("--build-script", help="Build script path")
    ap.add_argument("--hpo-num-samples", type=int, default=2)
    ap.add_argument("--force", action="store_true")
    ap.add_argument("--prediction-col", default="pred_readmit")
    ap.add_argument("--tmp-dir", default="/tmp/monitor")
    
    args = ap.parse_args()
    
    os.makedirs(args.tmp_dir, exist_ok=True)
    
    print("ğŸ”„ Loading data...")
    ref = pd.read_csv(args.baseline)
    cur = pd.read_csv(args.current)
    
    print(f"ğŸ“Š Reference data: {len(ref)} rows, {len(ref.columns)} columns")
    print(f"ğŸ“Š Current data: {len(cur)} rows, {len(cur.columns)} columns")
    
    # Score both datasets with preprocessing
    print(f"ğŸ¯ Scoring reference data against ML service at {args.endpoint}...")
    ref_scored = score_with_preprocessing(
        ref, 
        args.endpoint, 
        os.path.join(args.tmp_dir, "ref_processed.csv"),
        args.prediction_col
    )
    
    print(f"ğŸ¯ Scoring current data against ML service at {args.endpoint}...")
    cur_scored = score_with_preprocessing(
        cur, 
        args.endpoint, 
        os.path.join(args.tmp_dir, "cur_processed.csv"),
        args.prediction_col
    )
    
    # Use the original drift detection logic
    print("ğŸ” Detecting data drift...")
    ignore = {args.prediction_col}
    critical = set()
    
    trigger, summary = monitor_and_retrain.detect_drift(
        ref_scored, cur_scored,
        feature_psi_thresh=0.1,
        ks_p_thresh=0.05,
        drift_share_thresh=0.3,
        pred_psi_thresh=0.1,
        pred_col=args.prediction_col,
        ignore=ignore,
        critical=critical
    )
    
    print("\n" + "="*60)
    print("ğŸ“ˆ DRIFT DETECTION RESULTS")
    print("="*60)
    print(f"ğŸš¨ Drift detected: {trigger}")
    print(f"ğŸ“Š Feature drift share: {summary.get('drift_share', 0.0):.1%}")
    print(f"âš¡ Max PSI: {summary.get('max_psi', 0.0):.3f}")
    
    if summary.get('feature_results'):
        print(f"\nğŸ”¢ Feature Analysis ({len(summary['feature_results'])} features):")
        drift_count = sum(1 for r in summary['feature_results'] if r['drift'])
        print(f"   ğŸ“Š Features with drift: {drift_count}/{len(summary['feature_results'])}")
        
        # Show top 5 features by PSI
        sorted_features = sorted(summary['feature_results'], key=lambda x: x['psi'], reverse=True)[:5]
        for result in sorted_features:
            status = "ğŸ”´ DRIFT" if result['drift'] else "ğŸŸ¢ OK"
            print(f"  {status} {result['column']}: PSI={result['psi']:.3f}")
    
    if summary.get('pred_psi') is not None:
        print(f"\nğŸ¯ Prediction Analysis:")
        print(f"  PSI: {summary['pred_psi']:.3f}")
        pred_gate = summary.get('pred_gate', False)
        print(f"  Gate: {'ğŸ”´ TRIGGERED' if pred_gate else 'ğŸŸ¢ OK'}")
    
    print("\n" + "="*60)
    
    if args.force or trigger:
        print("ğŸš€ Drift detected or forced - would trigger retraining")
        print(f"   Retrain script: {args.retrain_script}")
        print(f"   Build script: {args.build_script}")
        print("   (Retraining disabled in demo mode)")
    else:
        print("âœ… No significant drift detected - no retraining needed")
    
    print("\nâœ… Real monitoring completed successfully!")

if __name__ == "__main__":
    main()